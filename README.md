# Azure AI Avatar Conversational Bot

A real-time avatar-based conversational bot powered by Azure OpenAI GPT-4o and Azure Speech Services with speech-to-text, text-to-speech, and 3D avatar integration.

## üöÄ Features

- **Azure OpenAI Integration**: Uses GPT-4o model deployed on Azure
- **Azure Speech Services**: Professional speech-to-text recognition
- **Real-time Speech Processing**: Sub-3 second response times
- **3D Avatar**: Emotional expressions and lip sync
- **Voice Interaction**: Speech-to-text and text-to-speech
- **Conversation History**: Persistent chat history
- **Emotion Analysis**: Real-time emotion detection
- **Audio Visualization**: Visual feedback for audio levels

## üõ† Setup

### Prerequisites

- Node.js 18+
- Azure OpenAI Service account
- Azure Speech Services account
- Modern browser with microphone access

### Installation

1. **Clone and install dependencies**:
\`\`\`bash
git clone <repository-url>
cd avatar-conversational-bot
npm install
\`\`\`

2. **Configure environment variables**:
Create `.env.local` with your Azure credentials:
\`\`\`env
# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://openaiservices-gosign.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview
AZURE_OPENAI_API_KEY=your-azure-openai-api-key
DEPLOYMENT_NAME=gpt-4o-pilot-ai-production

# Azure Speech Services Configuration
AZURE_SPEECH_ENDPOINT=https://swedencentral.api.cognitive.microsoft.com/
SPEECH_KEY=your-azure-speech-key
\`\`\`

3. **Start development server**:
\`\`\`bash
npm run dev
\`\`\`

4. **Open browser**:
Navigate to `http://localhost:3000`

## üèó Architecture

### Frontend Components
- **AvatarCanvas**: 3D avatar with Three.js
- **AudioVisualizer**: Real-time audio level display
- **ConversationHistory**: Chat message history
- **useConversation**: Main conversation logic hook

### Backend APIs
- **`/api/chat`**: Azure OpenAI chat completion
- **`/api/speech`**: Azure Speech Services transcription
- **`/api/test-speech`**: Configuration testing

### Key Technologies
- **Next.js 14**: Full-stack React framework
- **Azure OpenAI**: GPT-4o language model
- **Azure Speech Services**: Professional speech recognition
- **Three.js**: 3D graphics and avatar
- **Web Speech API**: Browser speech recognition (fallback)
- **AI SDK**: Unified AI model interface

## üéØ Azure Services Configuration

### Azure OpenAI
- **Endpoint**: `openaiservices-gosign.openai.azure.com`
- **Deployment**: `gpt-4o-pilot-ai-production`
- **API Version**: `2025-01-01-preview`
- **Model**: GPT-4o with enhanced emotional intelligence

### Azure Speech Services
- **Endpoint**: `swedencentral.api.cognitive.microsoft.com`
- **Region**: Sweden Central
- **Authentication**: Subscription key
- **Format**: 16-bit PCM WAV, 16kHz sample rate

## üîß Advanced Features

### Dual Speech Recognition
1. **Web Speech API** (primary): Fast, browser-native recognition
2. **Azure Speech Services** (fallback): Professional-grade accuracy
3. **Smart switching**: Uses Azure when Web Speech fails

### Audio Processing Pipeline
1. **Record audio** with optimized settings (16kHz, mono)
2. **Convert to WAV** format with proper encoding
3. **Send to Azure Speech Services** with error handling
4. **Process results** and display in conversation

### Emotion Detection
The system analyzes text responses to determine emotional context:
- **Happy**: Positive keywords and expressions
- **Sad**: Sympathetic or disappointing content
- **Excited**: Enthusiastic language
- **Concerned**: Cautionary or worried tone
- **Neutral**: Default emotional state

### Avatar Expressions
3D avatar responds with:
- **Color changes** based on emotion
- **Blinking animations** for liveliness
- **Head movements** during speech
- **Lip sync simulation** (basic implementation)

## üìä API Endpoints

### POST /api/chat
Process conversation with Azure OpenAI
\`\`\`json
{
  "messages": [
    {"role": "user", "content": "Hello"}
  ]
}
\`\`\`

### POST /api/speech
Convert speech to text using Azure Speech Services
\`\`\`json
{
  "audio": "audio_wav_data"
}
\`\`\`

Response:
\`\`\`json
{
  "text": "transcribed text",
  "confidence": 0.95
}
\`\`\`

### GET /api/test-speech
Test Azure Speech Services configuration
\`\`\`json
{
  "isConnected": true,
  "endpoint": "https://swedencentral.api.cognitive.microsoft.com/",
  "hasKey": true
}
\`\`\`

## üöÄ Deployment

### Vercel Deployment
\`\`\`bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel --prod
\`\`\`

Add environment variables in Vercel dashboard:
- `AZURE_OPENAI_ENDPOINT`
- `AZURE_OPENAI_API_KEY`
- `DEPLOYMENT_NAME`
- `AZURE_SPEECH_ENDPOINT`
- `SPEECH_KEY`

### Docker Deployment
\`\`\`bash
# Build image
docker build -t avatar-bot .

# Run container
docker run -p 3000:3000 --env-file .env.local avatar-bot
\`\`\`

## üß™ Testing

### Configuration Testing
1. Click "Test Speech Config" to verify Azure Speech Services setup
2. Check console logs for detailed debugging information
3. Verify API connectivity and available models

### Manual Testing
1. Click "Start Listening"
2. Speak into microphone
3. Observe avatar response and emotion changes
4. Check conversation history

## üîí Security

- API keys stored in environment variables
- CORS protection on API routes
- Input validation and sanitization
- Rate limiting (recommended for production)

## üìà Performance Metrics

- **Response Time**: < 3 seconds end-to-end
- **Audio Latency**: < 500ms for speech recognition
- **Avatar Rendering**: 60 FPS at 1080p
- **Memory Usage**: < 200MB typical

## üõ† Customization

### Adding Custom TTS
Replace Web Speech API with Azure Speech Services TTS:
\`\`\`typescript
const synthesizeSpeech = async (text: string) => {
  const response = await fetch('/api/azure-tts', {
    method: 'POST',
    body: JSON.stringify({ text, voice: 'en-US-AriaNeural' })
  });
  return response.blob();
};
\`\`\`

### Blender Avatar Integration
1. Export Blender model as GLTF/GLB
2. Add to `public/models/` directory
3. Update `AvatarCanvas` component:
\`\`\`typescript
import { useGLTF } from '@react-three/drei';

function BlenderAvatar() {
  const { scene } = useGLTF('/models/avatar.glb');
  return <primitive object={scene} />;
}
\`\`\`

## üìù License

MIT License - see LICENSE file for details.

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## üìû Support

For issues and questions:
- Create GitHub issue
- Check documentation
- Review Azure services status
